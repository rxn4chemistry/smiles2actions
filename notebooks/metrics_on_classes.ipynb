{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import cycle\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import attr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from paragraph2actions.analysis import partial_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smiles2actions.utils import load_list_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate metrics according to the reaction classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2a_dir = Path(os.environ['S2A_PAPER_DATA_DIR'])\n",
    "tgt_file = str(s2a_dir / 'tgt-test.txt')\n",
    "classes_file = str(s2a_dir / 'rxn_classes_test.txt')\n",
    "transformer_file = str(s2a_dir / 'transformer_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading samples and subdivide into classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxn_classes = load_list_from_file(classes_file)\n",
    "truths = load_list_from_file(tgt_file)\n",
    "preds = load_list_from_file(transformer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s(auto_attribs=True)\n",
    "class Sample:\n",
    "    rxn_class: str\n",
    "    truth: str\n",
    "    pred: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(truths) == len(rxn_classes) == len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = [\n",
    "    Sample(rxn_class, truth, pred) for rxn_class, truth, pred in zip(rxn_classes, truths, preds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_class: List[List[Sample]] = [[] for _ in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in all_samples:\n",
    "    superclass = int(sample.rxn_class.split('.')[0])\n",
    "    samples_per_class[superclass].append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = [50, 60, 70, 80, 90, 100]\n",
    "percentage_labels = [f'{percentage}% accuracy' for percentage in percentages]\n",
    "line_labels = [f'Superclass {i}' for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics on all the data')\n",
    "print(' - 100% accuracy', partial_accuracy(truths, preds, 1.0))\n",
    "print(' - 90% accuracy', partial_accuracy(truths, preds, 0.9))\n",
    "print(' - 75% accuracy', partial_accuracy(truths, preds, 0.75))\n",
    "print(' - 50% accuracy', partial_accuracy(truths, preds, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics for classes')\n",
    "results = np.zeros((12, len(percentages)))\n",
    "for superclass_index, class_samples in enumerate(samples_per_class):\n",
    "    truth_for_class = [sample.truth for sample in class_samples]\n",
    "    pred_for_class = [sample.pred for sample in class_samples]\n",
    "    for percentage_index, percentage in enumerate(percentages):\n",
    "        acc = partial_accuracy(truth_for_class, pred_for_class, percentage / 100)\n",
    "        results[superclass_index, percentage_index] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\"-\", \"--\", \":\"]\n",
    "linecycler = cycle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
    "for y_arr, label in zip(100 * results, list(range(12))):\n",
    "    ax.plot(percentages, y_arr, next(linecycler), label=label)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylabel('Score (in %)')\n",
    "ax.set_xticks(percentages)\n",
    "ax.set_xticklabels(percentage_labels, rotation=20)\n",
    "fig.tight_layout()\n",
    "plt.savefig('/tmp/metrics_per_class.pdf')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
